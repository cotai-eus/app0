"""
üéØ Valida√ß√£o Completa do Sistema LLM CotAi
============================================

Este script executa uma valida√ß√£o abrangente de todo o sistema LLM
implementado para o CotAi Backend, validando desde a infraestrutura 
at√© a funcionalidade completa dos servi√ßos de IA.
"""

import asyncio
import sys
import httpx
import json
from pathlib import Path
from datetime import datetime


class LLMSystemValidator:
    """Validador completo do sistema LLM"""
    
    def __init__(self):
        self.results = {
            "infrastructure": {},
            "services": {},
            "models": {},
            "functionality": {},
            "performance": {}
        }
        self.total_tests = 0
        self.passed_tests = 0
        self.failed_tests = 0
    
    def log_result(self, category: str, test_name: str, success: bool, details: str = ""):
        """Registra resultado de um teste"""
        self.total_tests += 1
        if success:
            self.passed_tests += 1
            status = "‚úÖ PASS"
        else:
            self.failed_tests += 1
            status = "‚ùå FAIL"
        
        self.results[category][test_name] = {
            "status": status,
            "success": success,
            "details": details,
            "timestamp": datetime.now().isoformat()
        }
        print(f"{status} {test_name}: {details}")
    
    def validate_infrastructure(self):
        """Valida infraestrutura do sistema LLM"""
        print("\nüèóÔ∏è  VALIDANDO INFRAESTRUTURA LLM")
        print("=" * 50)
        
        # Verificar estrutura de diret√≥rios
        directories = [
            "llm/",
            "llm/services/", 
            "backend/app/core/",
            "backend/requirements.txt"
        ]
        
        for directory in directories:
            path = Path(directory)
            exists = path.exists()
            self.log_result("infrastructure", f"Directory {directory}", exists, 
                          f"Path exists: {path.absolute()}")
        
        # Verificar arquivos principais
        files = [
            "llm/__init__.py",
            "llm/manager.py",
            "llm/models.py",
            "llm/api.py",
            "llm/services/ai_processing.py",
            "llm/services/text_extraction.py",
            "llm/services/cache.py",
            "docker-compose.yml"
        ]
        
        for file_path in files:
            path = Path(file_path)
            exists = path.exists()
            size = path.stat().st_size if exists else 0
            self.log_result("infrastructure", f"File {file_path}", exists,
                          f"Size: {size} bytes")
    
    async def validate_ollama_service(self):
        """Valida servi√ßo Ollama"""
        print("\nü§ñ VALIDANDO SERVI√áO OLLAMA")
        print("=" * 50)
        
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                # Teste de conectividade
                response = await client.get("http://localhost:11434/api/tags")
                if response.status_code == 200:
                    data = response.json()
                    models = [m['name'] for m in data.get('models', [])]
                    self.log_result("services", "Ollama Connection", True,
                                  f"Connected, Models: {models}")
                    
                    # Verificar modelos necess√°rios
                    required_models = ["llama3:8b", "llama3.2:3b"]
                    for model in required_models:
                        has_model = any(model in m for m in models)
                        self.log_result("models", f"Model {model}", has_model,
                                      f"Available: {has_model}")
                    
                    return True
                else:
                    self.log_result("services", "Ollama Connection", False,
                                  f"HTTP {response.status_code}")
                    return False
                    
        except Exception as e:
            self.log_result("services", "Ollama Connection", False, str(e))
            return False
    
    async def validate_llm_functionality(self):
        """Valida funcionalidade do LLM"""
        print("\nüß† VALIDANDO FUNCIONALIDADE LLM")
        print("=" * 50)
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                # Teste de gera√ß√£o simples
                payload = {
                    "model": "llama3.2:3b",
                    "prompt": "Responda apenas: OK",
                    "stream": False
                }
                
                response = await client.post("http://localhost:11434/api/generate", 
                                           json=payload)
                
                if response.status_code == 200:
                    data = response.json()
                    response_text = data.get('response', '').strip()
                    self.log_result("functionality", "Simple Generation", True,
                                  f"Response: '{response_text}'")
                    
                    # Teste de extra√ß√£o de dados estruturados
                    structured_prompt = """
                    Extraia as seguintes informa√ß√µes em formato JSON:
                    Documento: "Licita√ß√£o 123/2024 - Fornecimento de computadores"
                    Formato: {"numero": "123/2024", "objeto": "Fornecimento de computadores"}
                    """
                    
                    payload["prompt"] = structured_prompt
                    response = await client.post("http://localhost:11434/api/generate",
                                               json=payload)
                    
                    if response.status_code == 200:
                        data = response.json()
                        response_text = data.get('response', '')
                        
                        # Verificar se consegue extrair JSON
                        try:
                            # Procurar por JSON na resposta
                            if "{" in response_text and "}" in response_text:
                                self.log_result("functionality", "Structured Extraction", True,
                                              "JSON structure detected in response")
                            else:
                                self.log_result("functionality", "Structured Extraction", False,
                                              "No JSON structure found")
                        except:
                            self.log_result("functionality", "Structured Extraction", False,
                                          "Failed to parse structured response")
                    
                    return True
                else:
                    self.log_result("functionality", "Simple Generation", False,
                                  f"HTTP {response.status_code}")
                    return False
                    
        except Exception as e:
            self.log_result("functionality", "LLM Generation", False, str(e))
            return False
    
    async def validate_performance(self):
        """Valida performance do sistema"""
        print("\n‚ö° VALIDANDO PERFORMANCE")
        print("=" * 50)
        
        try:
            start_time = datetime.now()
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                payload = {
                    "model": "llama3.2:3b",
                    "prompt": "Gere uma lista de 5 itens para licita√ß√£o de material de escrit√≥rio.",
                    "stream": False
                }
                
                response = await client.post("http://localhost:11434/api/generate",
                                           json=payload)
                
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                
                if response.status_code == 200:
                    data = response.json()
                    response_length = len(data.get('response', ''))
                    
                    # Crit√©rios de performance
                    fast_response = duration < 30.0
                    adequate_length = response_length > 50
                    
                    self.log_result("performance", "Response Time", fast_response,
                                  f"{duration:.2f}s (target: <30s)")
                    self.log_result("performance", "Response Quality", adequate_length,
                                  f"{response_length} chars")
                    
                    return fast_response and adequate_length
                else:
                    self.log_result("performance", "Performance Test", False,
                                  f"HTTP {response.status_code}")
                    return False
                    
        except Exception as e:
            self.log_result("performance", "Performance Test", False, str(e))
            return False
    
    def generate_report(self):
        """Gera relat√≥rio final"""
        print("\n" + "=" * 60)
        print("üìä RELAT√ìRIO FINAL DE VALIDA√á√ÉO LLM")
        print("=" * 60)
        
        print(f"üìà Total de Testes: {self.total_tests}")
        print(f"‚úÖ Sucessos: {self.passed_tests}")
        print(f"‚ùå Falhas: {self.failed_tests}")
        print(f"üìä Taxa de Sucesso: {(self.passed_tests/self.total_tests*100):.1f}%")
        
        print("\nüìã DETALHES POR CATEGORIA:")
        
        for category, tests in self.results.items():
            category_passed = sum(1 for test in tests.values() if test['success'])
            category_total = len(tests)
            
            if category_total > 0:
                print(f"\n{category.upper()}:")
                print(f"  Sucessos: {category_passed}/{category_total}")
                
                for test_name, result in tests.items():
                    print(f"  {result['status']} {test_name}")
                    if result['details']:
                        print(f"     ‚Üí {result['details']}")
        
        # Determinar status geral
        critical_failures = []
        
        # Verificar falhas cr√≠ticas
        for category, tests in self.results.items():
            for test_name, result in tests.items():
                if not result['success']:
                    if 'Connection' in test_name or 'Model' in test_name:
                        critical_failures.append(f"{category}.{test_name}")
        
        print("\n" + "=" * 60)
        
        if not critical_failures:
            if self.failed_tests == 0:
                print("üéâ SISTEMA LLM COMPLETAMENTE VALIDADO!")
                print("‚úÖ Todos os testes passaram com sucesso")
                print("‚úÖ Sistema pronto para produ√ß√£o")
                status = "SUCCESS"
            else:
                print("‚ö†Ô∏è  SISTEMA LLM PARCIALMENTE VALIDADO")
                print("‚úÖ Funcionalidades principais operacionais")
                print("‚ö†Ô∏è  Algumas funcionalidades secund√°rias com problemas")
                status = "PARTIAL"
        else:
            print("‚ùå FALHA NA VALIDA√á√ÉO DO SISTEMA LLM")
            print("‚ùå Problemas cr√≠ticos encontrados:")
            for failure in critical_failures:
                print(f"   ‚Ä¢ {failure}")
            print("\nüîß A√ß√µes necess√°rias:")
            print("   1. Verificar se Ollama est√° rodando")
            print("   2. Verificar modelos dispon√≠veis")
            print("   3. Verificar conectividade de rede")
            status = "FAILED"
        
        # Salvar relat√≥rio
        report_file = f"llm_validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "status": status,
                "summary": {
                    "total_tests": self.total_tests,
                    "passed_tests": self.passed_tests,
                    "failed_tests": self.failed_tests,
                    "success_rate": self.passed_tests/self.total_tests*100 if self.total_tests > 0 else 0
                },
                "results": self.results,
                "critical_failures": critical_failures
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nüìÑ Relat√≥rio salvo em: {report_file}")
        print("=" * 60)
        
        return status == "SUCCESS"


async def main():
    """Fun√ß√£o principal de valida√ß√£o"""
    print("üéØ INICIANDO VALIDA√á√ÉO COMPLETA DO SISTEMA LLM")
    print("=" * 60)
    print(f"üïê Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("üè∑Ô∏è  Vers√£o: 1.0.0")
    print("üì¶ Sistema: CotAi Backend LLM Integration")
    
    validator = LLMSystemValidator()
    
    # Executar valida√ß√µes
    validator.validate_infrastructure()
    
    ollama_ok = await validator.validate_ollama_service()
    
    if ollama_ok:
        await validator.validate_llm_functionality()
        await validator.validate_performance()
    else:
        print("\n‚ö†Ô∏è  Ollama n√£o est√° dispon√≠vel - pulando testes funcionais")
    
    # Gerar relat√≥rio final
    success = validator.generate_report()
    
    return success


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Valida√ß√£o interrompida pelo usu√°rio")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n‚ùå Erro durante valida√ß√£o: {e}")
        sys.exit(1)
