# LLM Configuration Environment Variables
# Add these to your .env file or environment

# === Ollama Configuration ===
OLLAMA_API_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama3:8b
OLLAMA_TIMEOUT=300.0
OLLAMA_MAX_RETRIES=3
OLLAMA_RETRY_DELAY=2.0

# === GPU and Performance ===
OLLAMA_GPU_LAYERS=35
OLLAMA_CONTEXT_LENGTH=4096
OLLAMA_THREADS=8
OLLAMA_TEMPERATURE=0.1

# === Document Processing ===
MAX_DOCUMENT_SIZE_MB=50
TEXT_EXTRACTION_TIMEOUT=120.0
CHUNK_SIZE_TOKENS=3000
CHUNK_OVERLAP_TOKENS=200

# === Prompts and Models ===
PROMPT_VERSION=v1.0
PROMPT_TEMPLATES_PATH=app/ai/prompts
FALLBACK_MODEL=llama3:8b

# === Monitoring and Logging ===
AI_METRICS_ENABLED=true
AI_RESPONSE_TIME_THRESHOLD=60.0
AI_LOG_PROMPTS=true
AI_LOG_RESPONSES=true

# === Rate Limiting ===
AI_RATE_LIMIT_PER_MINUTE=30
AI_RATE_LIMIT_PER_HOUR=500
AI_CONCURRENT_REQUESTS=3

# === Cache Configuration ===
AI_CACHE_TTL_HOURS=24
AI_CACHE_ENABLED=true

# === Monitoring Configuration ===
AI_METRICS_RETENTION_DAYS=30
AI_PROCESSING_TIMEOUT=300.0

# === Celery Configuration for LLM ===
# Add these to your existing Celery configuration
CELERY_TASK_ROUTES_LLM=app.tasks.llm_tasks.*:llm_tasks
